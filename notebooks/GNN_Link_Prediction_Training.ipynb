{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GNN Link Prediction Training\n",
                "\n",
                "Train a Graph Neural Network (GCN) on the ViMed medical knowledge graph\n",
                "for **link prediction** -- predicting missing relations between medical entities.\n",
                "\n",
                "## Pipeline\n",
                "1. Load the NetworkX graph from pickle\n",
                "2. Engineer node/edge features\n",
                "3. Convert to PyTorch Geometric Data\n",
                "4. Define GCN encoder + link decoder\n",
                "5. Train with binary cross-entropy\n",
                "6. Evaluate with AUC & Average Precision\n",
                "7. Save trained model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pickle\n",
                "import numpy as np\n",
                "import networkx as nx\n",
                "import torch\n",
                "import torch.nn.functional as F\n",
                "from torch_geometric.data import Data\n",
                "from torch_geometric.nn import GCNConv\n",
                "from torch_geometric.transforms import RandomLinkSplit\n",
                "from sklearn.metrics import roc_auc_score, average_precision_score\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "\n",
                "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load & Explore the Medical Knowledge Graph"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Path to the graph pickle file\n",
                "GRAPH_PATH = os.path.join(\"..\", \"amg_data\", \"graph_improved.pkl\")\n",
                "MODEL_SAVE_PATH = os.path.join(\"..\", \"amg_data\", \"gnn_link_predictor.pt\")\n",
                "\n",
                "with open(GRAPH_PATH, \"rb\") as f:\n",
                "    G = pickle.load(f)\n",
                "\n",
                "print(f\"Graph type: {type(G).__name__}\")\n",
                "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
                "print(f\"Number of edges: {G.number_of_edges()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze node types\n",
                "node_types = {}\n",
                "for node, data in G.nodes(data=True):\n",
                "    ntype = data.get(\"type\", \"UNKNOWN\")\n",
                "    node_types[ntype] = node_types.get(ntype, 0) + 1\n",
                "\n",
                "print(\"\\n--- Node type distribution ---\")\n",
                "for ntype, count in sorted(node_types.items(), key=lambda x: -x[1]):\n",
                "    print(f\"  {ntype}: {count}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze edge/relation types\n",
                "edge_types = {}\n",
                "for u, v, data in G.edges(data=True):\n",
                "    rel = data.get(\"relation\", \"UNKNOWN\")\n",
                "    edge_types[rel] = edge_types.get(rel, 0) + 1\n",
                "\n",
                "print(\"\\n--- Edge relation distribution ---\")\n",
                "for rel, count in sorted(edge_types.items(), key=lambda x: -x[1])[:20]:\n",
                "    print(f\"  {rel}: {count}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Feature Engineering\n",
                "\n",
                "- **Node features**: One-hot encoding of node type\n",
                "- **Edge labels**: Integer encoding of relation type (used for analysis, not training target)\n",
                "\n",
                "For link prediction we train on edge existence (binary classification)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build ordered node list and mappings\n",
                "nodes_list = list(G.nodes())\n",
                "node_to_idx = {node: idx for idx, node in enumerate(nodes_list)}\n",
                "num_nodes = len(nodes_list)\n",
                "\n",
                "# Build node type vocabulary\n",
                "all_node_types = sorted(set(\n",
                "    G.nodes[n].get(\"type\", \"UNKNOWN\") for n in nodes_list\n",
                "))\n",
                "type_to_idx = {t: i for i, t in enumerate(all_node_types)}\n",
                "num_node_types = len(all_node_types)\n",
                "\n",
                "print(f\"Node type vocabulary ({num_node_types} types): {all_node_types}\")\n",
                "\n",
                "# Create one-hot node feature matrix\n",
                "x = torch.zeros(num_nodes, num_node_types, dtype=torch.float)\n",
                "for node in nodes_list:\n",
                "    idx = node_to_idx[node]\n",
                "    ntype = G.nodes[node].get(\"type\", \"UNKNOWN\")\n",
                "    x[idx, type_to_idx[ntype]] = 1.0\n",
                "\n",
                "print(f\"Node feature matrix shape: {x.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build edge index (convert MultiDiGraph to simple directed edges)\n",
                "# Deduplicate edges for the same (u, v) pair\n",
                "seen_edges = set()\n",
                "edge_src = []\n",
                "edge_dst = []\n",
                "edge_confidences = []\n",
                "\n",
                "for u, v, data in G.edges(data=True):\n",
                "    u_idx = node_to_idx[u]\n",
                "    v_idx = node_to_idx[v]\n",
                "    if (u_idx, v_idx) not in seen_edges:\n",
                "        seen_edges.add((u_idx, v_idx))\n",
                "        edge_src.append(u_idx)\n",
                "        edge_dst.append(v_idx)\n",
                "        edge_confidences.append(data.get(\"confidence\", 0.5))\n",
                "\n",
                "edge_index = torch.tensor([edge_src, edge_dst], dtype=torch.long)\n",
                "edge_conf = torch.tensor(edge_confidences, dtype=torch.float)\n",
                "\n",
                "print(f\"Unique directed edges: {edge_index.shape[1]}\")\n",
                "print(f\"Edge confidence stats: mean={edge_conf.mean():.3f}, \"\n",
                "      f\"min={edge_conf.min():.3f}, max={edge_conf.max():.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Convert to PyTorch Geometric Data & Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create PyG Data object\n",
                "data = Data(x=x, edge_index=edge_index)\n",
                "data.num_nodes = num_nodes\n",
                "\n",
                "print(f\"PyG Data: {data}\")\n",
                "print(f\"  Nodes: {data.num_nodes}\")\n",
                "print(f\"  Edges: {data.num_edges}\")\n",
                "print(f\"  Node features dim: {data.num_node_features}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split edges into train / val / test\n",
                "# 85% train, 5% val, 10% test\n",
                "transform = RandomLinkSplit(\n",
                "    num_val=0.05,\n",
                "    num_test=0.10,\n",
                "    is_undirected=False,\n",
                "    add_negative_train_samples=True,\n",
                "    neg_sampling_ratio=1.0,\n",
                ")\n",
                "\n",
                "train_data, val_data, test_data = transform(data)\n",
                "\n",
                "print(f\"Train edges: {train_data.edge_label_index.shape[1]} \"\n",
                "      f\"(pos + neg, labels sum={train_data.edge_label.sum().int()})\")\n",
                "print(f\"Val   edges: {val_data.edge_label_index.shape[1]}\")\n",
                "print(f\"Test  edges: {test_data.edge_label_index.shape[1]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model Definition\n",
                "\n",
                "**Architecture:**\n",
                "- 2-layer GCN encoder that produces node embeddings\n",
                "- Dot-product decoder for link prediction scoring"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class GCNEncoder(torch.nn.Module):\n",
                "    \"\"\"Two-layer GCN producing node embeddings.\"\"\"\n",
                "\n",
                "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.3):\n",
                "        super().__init__()\n",
                "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
                "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
                "        self.dropout = dropout\n",
                "\n",
                "    def forward(self, x, edge_index):\n",
                "        x = self.conv1(x, edge_index)\n",
                "        x = F.relu(x)\n",
                "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
                "        x = self.conv2(x, edge_index)\n",
                "        return x\n",
                "\n",
                "\n",
                "class LinkPredictor(torch.nn.Module):\n",
                "    \"\"\"GCN encoder + dot-product link decoder.\"\"\"\n",
                "\n",
                "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.3):\n",
                "        super().__init__()\n",
                "        self.encoder = GCNEncoder(in_channels, hidden_channels, out_channels, dropout)\n",
                "\n",
                "    def encode(self, x, edge_index):\n",
                "        return self.encoder(x, edge_index)\n",
                "\n",
                "    def decode(self, z, edge_label_index):\n",
                "        \"\"\"Dot-product decoder: score = sigmoid(z_u . z_v).\"\"\"\n",
                "        src = z[edge_label_index[0]]\n",
                "        dst = z[edge_label_index[1]]\n",
                "        return (src * dst).sum(dim=-1)\n",
                "\n",
                "\n",
                "# Hyperparameters\n",
                "HIDDEN_DIM = 128\n",
                "EMBED_DIM = 64\n",
                "DROPOUT = 0.3\n",
                "LR = 0.01\n",
                "EPOCHS = 200\n",
                "\n",
                "model = LinkPredictor(\n",
                "    in_channels=num_node_types,\n",
                "    hidden_channels=HIDDEN_DIM,\n",
                "    out_channels=EMBED_DIM,\n",
                "    dropout=DROPOUT,\n",
                ").to(DEVICE)\n",
                "\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
                "\n",
                "print(model)\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "print(f\"\\nTotal parameters: {total_params:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(model, train_data, optimizer, device):\n",
                "    \"\"\"Run one training epoch. Returns the loss value.\"\"\"\n",
                "    model.train()\n",
                "    td = train_data.to(device)\n",
                "\n",
                "    optimizer.zero_grad()\n",
                "    z = model.encode(td.x, td.edge_index)\n",
                "    logits = model.decode(z, td.edge_label_index)\n",
                "    loss = F.binary_cross_entropy_with_logits(logits, td.edge_label)\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "\n",
                "    return loss.item()\n",
                "\n",
                "\n",
                "@torch.no_grad()\n",
                "def evaluate(model, eval_data, device):\n",
                "    \"\"\"Evaluate model on given data split. Returns AUC and AP.\"\"\"\n",
                "    model.eval()\n",
                "    ed = eval_data.to(device)\n",
                "\n",
                "    z = model.encode(ed.x, ed.edge_index)\n",
                "    logits = model.decode(z, ed.edge_label_index)\n",
                "    probs = torch.sigmoid(logits).cpu().numpy()\n",
                "    labels = ed.edge_label.cpu().numpy()\n",
                "\n",
                "    auc = roc_auc_score(labels, probs)\n",
                "    ap = average_precision_score(labels, probs)\n",
                "    return auc, ap"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training\n",
                "print(\"Starting training...\")\n",
                "print(f\"{'Epoch':>6} | {'Loss':>8} | {'Val AUC':>8} | {'Val AP':>8}\")\n",
                "print(\"-\" * 42)\n",
                "\n",
                "best_val_auc = 0.0\n",
                "history = {\"loss\": [], \"val_auc\": [], \"val_ap\": []}\n",
                "\n",
                "for epoch in range(1, EPOCHS + 1):\n",
                "    loss = train_epoch(model, train_data, optimizer, DEVICE)\n",
                "    history[\"loss\"].append(loss)\n",
                "\n",
                "    if epoch % 10 == 0 or epoch == 1:\n",
                "        val_auc, val_ap = evaluate(model, val_data, DEVICE)\n",
                "        history[\"val_auc\"].append(val_auc)\n",
                "        history[\"val_ap\"].append(val_ap)\n",
                "        print(f\"{epoch:6d} | {loss:8.4f} | {val_auc:8.4f} | {val_ap:8.4f}\")\n",
                "\n",
                "        # Save best model\n",
                "        if val_auc > best_val_auc:\n",
                "            best_val_auc = val_auc\n",
                "            torch.save({\n",
                "                \"model_state_dict\": model.state_dict(),\n",
                "                \"node_types\": all_node_types,\n",
                "                \"type_to_idx\": type_to_idx,\n",
                "                \"hidden_dim\": HIDDEN_DIM,\n",
                "                \"embed_dim\": EMBED_DIM,\n",
                "                \"num_node_types\": num_node_types,\n",
                "                \"epoch\": epoch,\n",
                "                \"val_auc\": val_auc,\n",
                "            }, MODEL_SAVE_PATH)\n",
                "\n",
                "print(f\"\\nTraining complete. Best validation AUC: {best_val_auc:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Training Curves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    import matplotlib.pyplot as plt\n",
                "\n",
                "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "    # Loss curve\n",
                "    ax1.plot(history[\"loss\"], color=\"#FF6B6B\", linewidth=1.5)\n",
                "    ax1.set_xlabel(\"Epoch\")\n",
                "    ax1.set_ylabel(\"BCE Loss\")\n",
                "    ax1.set_title(\"Training Loss\")\n",
                "    ax1.grid(True, alpha=0.3)\n",
                "\n",
                "    # AUC/AP curve\n",
                "    eval_epochs = list(range(1, len(history[\"val_auc\"]) + 1))\n",
                "    ax2.plot(eval_epochs, history[\"val_auc\"], label=\"AUC\", color=\"#4ECDC4\", linewidth=2)\n",
                "    ax2.plot(eval_epochs, history[\"val_ap\"], label=\"AP\", color=\"#FFE66D\", linewidth=2)\n",
                "    ax2.set_xlabel(\"Evaluation step\")\n",
                "    ax2.set_ylabel(\"Score\")\n",
                "    ax2.set_title(\"Validation Metrics\")\n",
                "    ax2.legend()\n",
                "    ax2.grid(True, alpha=0.3)\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "except ImportError:\n",
                "    print(\"matplotlib not installed. Skipping plots.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Test Set Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load best model and evaluate on test set\n",
                "checkpoint = torch.load(MODEL_SAVE_PATH, map_location=DEVICE, weights_only=False)\n",
                "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
                "\n",
                "test_auc, test_ap = evaluate(model, test_data, DEVICE)\n",
                "\n",
                "print(\"=\" * 40)\n",
                "print(\"       TEST SET RESULTS\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"  AUC:              {test_auc:.4f}\")\n",
                "print(f\"  Average Precision: {test_ap:.4f}\")\n",
                "print(f\"  Best epoch:        {checkpoint['epoch']}\")\n",
                "print(\"=\" * 40)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Detailed predictions on test edges\n",
                "model.eval()\n",
                "td = test_data.to(DEVICE)\n",
                "\n",
                "with torch.no_grad():\n",
                "    z = model.encode(td.x, td.edge_index)\n",
                "    test_logits = model.decode(z, td.edge_label_index)\n",
                "    test_probs = torch.sigmoid(test_logits).cpu().numpy()\n",
                "    test_labels = td.edge_label.cpu().numpy()\n",
                "\n",
                "# Count predictions by threshold\n",
                "threshold = 0.5\n",
                "preds = (test_probs >= threshold).astype(int)\n",
                "tp = ((preds == 1) & (test_labels == 1)).sum()\n",
                "fp = ((preds == 1) & (test_labels == 0)).sum()\n",
                "tn = ((preds == 0) & (test_labels == 0)).sum()\n",
                "fn = ((preds == 0) & (test_labels == 1)).sum()\n",
                "\n",
                "print(f\"\\nConfusion Matrix (threshold={threshold}):\")\n",
                "print(f\"  TP={tp}  FP={fp}\")\n",
                "print(f\"  FN={fn}  TN={tn}\")\n",
                "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
                "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
                "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
                "print(f\"  Precision: {precision:.4f}\")\n",
                "print(f\"  Recall:    {recall:.4f}\")\n",
                "print(f\"  F1 Score:  {f1:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Predict Missing Links\n",
                "\n",
                "Use the trained model to score node pairs that do NOT have an edge and find\n",
                "the most likely missing connections."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict top-K most likely missing links\n",
                "TOP_K = 20\n",
                "\n",
                "model.eval()\n",
                "full_data = data.to(DEVICE)\n",
                "\n",
                "with torch.no_grad():\n",
                "    z = model.encode(full_data.x, full_data.edge_index)\n",
                "\n",
                "# Existing edges as a set for fast lookup\n",
                "existing = set()\n",
                "ei = full_data.edge_index.cpu().numpy()\n",
                "for i in range(ei.shape[1]):\n",
                "    existing.add((ei[0, i], ei[1, i]))\n",
                "\n",
                "# Sample candidate pairs (full NxN is too large)\n",
                "# Focus on high-degree nodes for meaningful predictions\n",
                "degrees = dict(G.degree())\n",
                "top_nodes = sorted(degrees, key=degrees.get, reverse=True)[:50]\n",
                "top_indices = [node_to_idx[n] for n in top_nodes]\n",
                "\n",
                "candidates = []\n",
                "for i in top_indices:\n",
                "    for j in top_indices:\n",
                "        if i != j and (i, j) not in existing:\n",
                "            candidates.append((i, j))\n",
                "\n",
                "if candidates:\n",
                "    cand_src = torch.tensor([c[0] for c in candidates], dtype=torch.long)\n",
                "    cand_dst = torch.tensor([c[1] for c in candidates], dtype=torch.long)\n",
                "    cand_index = torch.stack([cand_src, cand_dst]).to(DEVICE)\n",
                "\n",
                "    with torch.no_grad():\n",
                "        scores = torch.sigmoid(model.decode(z, cand_index)).cpu().numpy()\n",
                "\n",
                "    # Get top-K predictions\n",
                "    top_k_idx = np.argsort(scores)[::-1][:TOP_K]\n",
                "\n",
                "    print(f\"\\nTop {TOP_K} predicted missing links:\")\n",
                "    print(f\"{'Source':<30} {'Target':<30} {'Score':>8}\")\n",
                "    print(\"-\" * 70)\n",
                "    for idx in top_k_idx:\n",
                "        src_node = nodes_list[candidates[idx][0]]\n",
                "        dst_node = nodes_list[candidates[idx][1]]\n",
                "        print(f\"{src_node:<30} {dst_node:<30} {scores[idx]:8.4f}\")\n",
                "else:\n",
                "    print(\"No candidate pairs found.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"\\nModel saved to: {os.path.abspath(MODEL_SAVE_PATH)}\")\n",
                "print(\"Training notebook complete.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}