{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === APPLY IMPROVEMENTS L√äN GRAPH HI·ªÜN C√ì (479 nodes) ===\n",
                "import pickle\n",
                "import os\n",
                "import re\n",
                "import unicodedata\n",
                "import networkx as nx\n",
                "from pyvis.network import Network\n",
                "import chromadb\n",
                "from chromadb.utils import embedding_functions\n",
                "\n",
                "print(\"üöÄ LOADING EXISTING GRAPH...\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === HELPER FUNCTIONS ===\n",
                "MEDICAL_ABBREVIATIONS = {\n",
                "    \"btm\": \"b·ªánh th·∫≠n m·∫°n\", \"tha\": \"tƒÉng huy·∫øt √°p\", \"ƒëtƒë\": \"ƒë√°i th√°o ƒë∆∞·ªùng\",\n",
                "    \"gfr\": \"ƒë·ªô l·ªçc c·∫ßu th·∫≠n\", \"egfr\": \"ƒë·ªô l·ªçc c·∫ßu th·∫≠n ∆∞·ªõc t√≠nh\",\n",
                "}\n",
                "\n",
                "MEDICAL_SYNONYMS = {\n",
                "    \"b·ªánh th·∫≠n m·∫°n\": [\"b·ªánh th·∫≠n m√£n\", \"suy th·∫≠n m·∫°n\", \"ckd\"],\n",
                "    \"ƒë√°i th√°o ƒë∆∞·ªùng\": [\"ti·ªÉu ƒë∆∞·ªùng\", \"ƒëtƒë\", \"diabetes\"],\n",
                "    \"tƒÉng huy·∫øt √°p\": [\"cao huy·∫øt √°p\", \"tha\", \"hypertension\"],\n",
                "}\n",
                "\n",
                "def normalize_medical_text(text: str) -> str:\n",
                "    if not text: return \"Unknown\"\n",
                "    text = unicodedata.normalize(\"NFC\", text).strip().lower()\n",
                "    text = re.sub(r'\\s+', ' ', text)\n",
                "    \n",
                "    words = text.split()\n",
                "    expanded = [MEDICAL_ABBREVIATIONS.get(re.sub(r'[^\\w]', '', w), w) for w in words]\n",
                "    text = ' '.join(expanded)\n",
                "    \n",
                "    for canonical, variants in MEDICAL_SYNONYMS.items():\n",
                "        for variant in variants:\n",
                "            text = text.replace(variant, canonical)\n",
                "    \n",
                "    return re.sub(r'\\s+', ' ', text).strip().title()\n",
                "\n",
                "print(\"‚úÖ Helper functions loaded.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === LOAD GRAPH CHECKPOINT ===\n",
                "graph_path = \"./amg_data/graph_full.pkl\"\n",
                "\n",
                "if os.path.exists(graph_path):\n",
                "    with open(graph_path, \"rb\") as f:\n",
                "        G = pickle.load(f)\n",
                "    print(f\"‚úÖ ƒê√£ load graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
                "else:\n",
                "    print(\"‚ùå Kh√¥ng t√¨m th·∫•y checkpoint!\")\n",
                "    raise FileNotFoundError(\"Ch·∫°y AMG_RAG_Enhanced.ipynb tr∆∞·ªõc ƒë·ªÉ t·∫°o checkpoint.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === LOAD VECTORDB ===\n",
                "chroma_path = \"./amg_data/chroma_db\"\n",
                "\n",
                "if os.path.exists(chroma_path):\n",
                "    client = chromadb.PersistentClient(path=chroma_path)\n",
                "    ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
                "        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
                "    )\n",
                "    col = client.get_collection(\"medical_chunks\", embedding_function=ef)\n",
                "    \n",
                "    # Get all documents ƒë·ªÉ t√≠nh frequency\n",
                "    all_docs = col.get()\n",
                "    all_chunks_text = [doc.lower() for doc in all_docs['documents']]\n",
                "    \n",
                "    print(f\"‚úÖ ƒê√£ load VectorDB: {len(all_chunks_text)} chunks\")\n",
                "else:\n",
                "    print(\"‚ùå Kh√¥ng t√¨m th·∫•y VectorDB!\")\n",
                "    all_chunks_text = []"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === IMPROVEMENT 1: ENTITY ENHANCEMENT T·ª™ VECTORDB ===\n",
                "print(\"\\nüîç IMPROVEMENT 1: Entity Enhancement t·ª´ VectorDB...\\n\")\n",
                "\n",
                "enhanced_count = 0\n",
                "\n",
                "for node in list(G.nodes):\n",
                "    label = G.nodes[node].get('label', node)\n",
                "    \n",
                "    # Query VectorDB cho entity\n",
                "    try:\n",
                "        results = col.query(query_texts=[label], n_results=2)\n",
                "        if results['documents'] and results['documents'][0]:\n",
                "            # Combine top 2 chunks\n",
                "            enhanced_context = \" \".join(results['documents'][0])[:300]\n",
                "            \n",
                "            # Update description\n",
                "            old_desc = G.nodes[node].get('description', '')\n",
                "            G.nodes[node]['description'] = f\"{old_desc}\\n[Enhanced]: {enhanced_context}\"\n",
                "            \n",
                "            # Boost confidence\n",
                "            old_conf = G.nodes[node].get('confidence', 0.5)\n",
                "            G.nodes[node]['confidence'] = min(1.0, old_conf + 0.15)\n",
                "            \n",
                "            enhanced_count += 1\n",
                "            if enhanced_count <= 5:  # Show first 5\n",
                "                print(f\"   ‚úÖ {label}: conf {old_conf:.2f} ‚Üí {G.nodes[node]['confidence']:.2f}\")\n",
                "    except Exception as e:\n",
                "        pass  # Skip n·∫øu l·ªói\n",
                "\n",
                "print(f\"\\n‚úÖ Enhanced {enhanced_count}/{G.number_of_nodes()} entities\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === IMPROVEMENT 2: FREQUENCY-BASED CONFIDENCE BOOST ===\n",
                "print(\"\\nüìä IMPROVEMENT 2: Frequency-based Confidence Boost...\\n\")\n",
                "\n",
                "def get_entity_frequency(entity_name: str, chunks: list) -> int:\n",
                "    \"\"\"ƒê·∫øm s·ªë l·∫ßn entity xu·∫•t hi·ªán\"\"\"\n",
                "    norm_name = normalize_medical_text(entity_name).lower()\n",
                "    return sum(1 for chunk in chunks if norm_name in chunk)\n",
                "\n",
                "boosted_count = 0\n",
                "\n",
                "for node in list(G.nodes):\n",
                "    label = G.nodes[node].get('label', node)\n",
                "    frequency = get_entity_frequency(label, all_chunks_text)\n",
                "    \n",
                "    if frequency >= 3:\n",
                "        # Calculate boost\n",
                "        boost = 0.1 if frequency >= 5 else 0.05\n",
                "        \n",
                "        # Apply boost\n",
                "        old_conf = G.nodes[node]['confidence']\n",
                "        G.nodes[node]['confidence'] = min(1.0, old_conf + boost)\n",
                "        \n",
                "        # Add frequency info to description\n",
                "        G.nodes[node]['description'] += f\" [Freq: {frequency}x]\"\n",
                "        \n",
                "        boosted_count += 1\n",
                "        if boosted_count <= 10:\n",
                "            print(f\"   üìà {label}: {frequency}x ‚Üí conf {old_conf:.2f} ‚Üí {G.nodes[node]['confidence']:.2f}\")\n",
                "\n",
                "print(f\"\\n‚úÖ Boosted {boosted_count} entities (freq >= 3)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === IMPROVEMENT 3: DEDUPLICATION STATUS ===\n",
                "print(\"\\nüîÑ IMPROVEMENT 3: Deduplication Status...\\n\")\n",
                "\n",
                "# Check entities c√≥ multiple pages (ƒë√£ ƒë∆∞·ª£c merge)\n",
                "merged_entities = []\n",
                "for node in G.nodes:\n",
                "    pages = G.nodes[node].get('pages', [])\n",
                "    if len(pages) > 1:\n",
                "        merged_entities.append((G.nodes[node].get('label', node), len(pages)))\n",
                "\n",
                "merged_entities.sort(key=lambda x: x[1], reverse=True)\n",
                "\n",
                "print(f\"‚úÖ T√¨m th·∫•y {len(merged_entities)} entities ƒë√£ ƒë∆∞·ª£c merge t·ª´ nhi·ªÅu chunks\")\n",
                "print(\"\\nTop 10 merged entities:\")\n",
                "for entity, count in merged_entities[:10]:\n",
                "    print(f\"   - {entity}: xu·∫•t hi·ªán ·ªü {count} pages\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === SAVE IMPROVED GRAPH ===\n",
                "print(\"\\nüíæ SAVING IMPROVED GRAPH...\\n\")\n",
                "\n",
                "# Save checkpoint\n",
                "with open(\"./amg_data/graph_improved.pkl\", \"wb\") as f:\n",
                "    pickle.dump(G, f)\n",
                "\n",
                "print(f\"‚úÖ ƒê√£ l∆∞u: ./amg_data/graph_improved.pkl\")\n",
                "print(f\"   - {G.number_of_nodes()} nodes\")\n",
                "print(f\"   - {G.number_of_edges()} edges\")\n",
                "print(f\"   - {enhanced_count} entities enhanced\")\n",
                "print(f\"   - {boosted_count} entities boosted by frequency\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === VISUALIZE IMPROVED GRAPH ===\n",
                "print(\"\\nüìä CREATING VISUALIZATION...\\n\")\n",
                "\n",
                "net = Network(height=\"850px\", width=\"100%\", directed=True, notebook=False)\n",
                "net.force_atlas_2based(gravity=-50, spring_length=200)\n",
                "\n",
                "color_map = {\n",
                "    'DISEASE': '#ff6b6b', 'DRUG': '#4ecdc4', 'SYMPTOM': '#ffe66d',\n",
                "    'TEST': '#95e1d3', 'ANATOMY': '#f38181', 'TREATMENT': '#aa96da',\n",
                "    'PROCEDURE': '#a8d8ea', 'RISK_FACTOR': '#ffa07a', 'LAB_VALUE': '#98d8c8'\n",
                "}\n",
                "\n",
                "for node, data in G.nodes(data=True):\n",
                "    node_type = data.get('type', 'OTHER')\n",
                "    color = color_map.get(node_type, '#97C2FC')\n",
                "    conf = data.get('confidence', 0.5)\n",
                "    size = 15 + conf * 25  # Bigger nodes for higher confidence\n",
                "    \n",
                "    title = f\"<b>{data.get('label')}</b><br>Type: {node_type}<br>Conf: {conf:.2f}<br>Pages: {data.get('pages')}\"\n",
                "    net.add_node(node, label=data.get('label'), title=title, color=color, size=size)\n",
                "\n",
                "for u, v, data in G.edges(data=True):\n",
                "    conf = data.get('confidence', 0.5)\n",
                "    width = 1 + conf * 3\n",
                "    net.add_edge(u, v, label=data.get('relation'), title=data.get('evidence', ''), width=width)\n",
                "\n",
                "net.show_buttons(filter_=['physics'])\n",
                "net.write_html(\"Medical_Graph_Improved_479.html\")\n",
                "\n",
                "print(\"‚úÖ ƒê√£ l∆∞u visualization: Medical_Graph_Improved_479.html\")\n",
                "print(\"\\nüéâ HO√ÄN T·∫§T! M·ªü file HTML ƒë·ªÉ xem graph c·∫£i ti·∫øn.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === SO S√ÅNH TR∆Ø·ªöC/SAU ===\n",
                "print(\"\\nüìä SO S√ÅNH CONFIDENCE SCORES...\\n\")\n",
                "\n",
                "# Sample 10 nodes ƒë·ªÉ xem s·ª± thay ƒë·ªïi\n",
                "import random\n",
                "sample_nodes = random.sample(list(G.nodes), min(10, len(G.nodes)))\n",
                "\n",
                "print(\"Sample confidence scores:\")\n",
                "for node in sample_nodes:\n",
                "    label = G.nodes[node].get('label', node)\n",
                "    conf = G.nodes[node].get('confidence', 0)\n",
                "    rel_score = G.nodes[node].get('relevance_score', 0)\n",
                "    pages = len(G.nodes[node].get('pages', []))\n",
                "    print(f\"   - {label[:30]:30s} | Conf: {conf:.2f} | Rel: {rel_score} | Pages: {pages}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}