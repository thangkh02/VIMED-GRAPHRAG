{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === RESUME EXTRACTION - CH·ªà TR√çCH TH·ª∞C TH·ªÇ ===\n",
                "!pip install -q langchain-groq pyvis pypdf networkx pydantic python-dotenv\n",
                "\n",
                "import os\n",
                "import re\n",
                "import time\n",
                "import pickle\n",
                "import unicodedata\n",
                "import networkx as nx\n",
                "from typing import List\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "from langchain_community.document_loaders import PyPDFLoader\n",
                "from langchain_text_splitters import TokenTextSplitter\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_groq import ChatGroq\n",
                "from pyvis.network import Network\n",
                "from pydantic import BaseModel, Field\n",
                "\n",
                "print(\"‚úÖ Import OK\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === C·∫§U H√åNH ===\n",
                "load_dotenv()\n",
                "\n",
                "GROQ_API_KEYS = [\n",
                "    os.getenv(\"GROQ_API_KEY_1\"),\n",
                "    os.getenv(\"GROQ_API_KEY_2\"),\n",
                "    os.getenv(\"GROQ_API_KEY_3\"),\n",
                "    os.getenv(\"GROQ_API_KEY_4\"),\n",
                "    os.getenv(\"GROQ_API_KEY_5\"),\n",
                "    os.getenv(\"GROQ_API_KEY_6\"),\n",
                "    os.getenv(\"GROQ_API_KEY_7\"),\n",
                "    os.getenv(\"GROQ_API_KEY_8\"),\n",
                "    os.getenv(\"GROQ_API_KEY_9\"),\n",
                "]\n",
                "PDF_PATH = os.getenv(\"PDF_PATH\")\n",
                "\n",
                "# Normalize\n",
                "MEDICAL_ABBREVIATIONS = {\n",
                "    \"btm\": \"b·ªánh th·∫≠n m·∫°n\", \"tha\": \"tƒÉng huy·∫øt √°p\", \"ƒëtƒë\": \"ƒë√°i th√°o ƒë∆∞·ªùng\",\n",
                "    \"gfr\": \"ƒë·ªô l·ªçc c·∫ßu th·∫≠n\", \"ckd\": \"b·ªánh th·∫≠n m·∫°n\",\n",
                "}\n",
                "\n",
                "MEDICAL_SYNONYMS = {\n",
                "    \"b·ªánh th·∫≠n m·∫°n\": [\"b·ªánh th·∫≠n m√£n\", \"suy th·∫≠n m·∫°n\", \"ckd\"],\n",
                "    \"ƒë√°i th√°o ƒë∆∞·ªùng\": [\"ti·ªÉu ƒë∆∞·ªùng\", \"ƒëtƒë\", \"diabetes\"],\n",
                "}\n",
                "\n",
                "def normalize_medical_text(text: str) -> str:\n",
                "    if not text: return \"Unknown\"\n",
                "    text = unicodedata.normalize(\"NFC\", text).strip().lower()\n",
                "    text = re.sub(r'\\s+', ' ', text)\n",
                "    words = [MEDICAL_ABBREVIATIONS.get(re.sub(r'[^\\w]', '', w), w) for w in text.split()]\n",
                "    text = ' '.join(words)\n",
                "    for canonical, variants in MEDICAL_SYNONYMS.items():\n",
                "        for variant in variants:\n",
                "            text = text.replace(variant, canonical)\n",
                "    return re.sub(r'\\s+', ' ', text).strip().title()\n",
                "\n",
                "class APIKeyManager:\n",
                "    def __init__(self, api_keys: List[str]):\n",
                "        self.api_keys = [k for k in api_keys if k]\n",
                "        self.current_index = 0\n",
                "        self.failed_keys = set()\n",
                "    def get_current_key(self) -> str:\n",
                "        return self.api_keys[self.current_index]\n",
                "    def rotate_key(self) -> bool:\n",
                "        self.failed_keys.add(self.current_index)\n",
                "        for i in range(len(self.api_keys)):\n",
                "            next_idx = (self.current_index + 1 + i) % len(self.api_keys)\n",
                "            if next_idx not in self.failed_keys:\n",
                "                self.current_index = next_idx\n",
                "                print(f\"üîÑ Key #{next_idx + 1}\")\n",
                "                return True\n",
                "        return False\n",
                "    def reset_failed(self):\n",
                "        self.failed_keys.clear()\n",
                "\n",
                "api_manager = APIKeyManager(GROQ_API_KEYS)\n",
                "print(f\"‚úÖ {len(api_manager.api_keys)} API keys\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === SCHEMA ===\n",
                "class MedicalEntity(BaseModel):\n",
                "    name: str\n",
                "    type: str\n",
                "    description: str = \"\"\n",
                "    relevance_score: int = Field(default=5, ge=1, le=10)\n",
                "\n",
                "class MedicalRelation(BaseModel):\n",
                "    source_name: str\n",
                "    target_name: str\n",
                "    relation: str\n",
                "    confidence_score: int = Field(default=5, ge=1, le=10)\n",
                "    evidence: str = \"\"\n",
                "\n",
                "class AMGExtractionResult(BaseModel):\n",
                "    entities: List[MedicalEntity] = Field(default_factory=list)\n",
                "    relations: List[MedicalRelation] = Field(default_factory=list)\n",
                "\n",
                "print(\"‚úÖ Schema OK\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === LOAD CHECKPOINT ===\n",
                "graph_path = \"./amg_data/graph_full.pkl\"\n",
                "\n",
                "if os.path.exists(graph_path):\n",
                "    with open(graph_path, \"rb\") as f:\n",
                "        G = pickle.load(f)\n",
                "    print(f\"‚úÖ Loaded: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y checkpoint, t·∫°o graph m·ªõi\")\n",
                "    G = nx.DiGraph()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === OPTIMIZED EXTRACTOR ===\n",
                "class OptimizedAMGExtractor:\n",
                "    def __init__(self, api_manager: APIKeyManager):\n",
                "        self.api_manager = api_manager\n",
                "        self.extraction_prompt = ChatPromptTemplate.from_messages([\n",
                "            (\"system\", \"\"\"B·∫°n l√† chuy√™n gia tr√≠ch xu·∫•t th·ª±c th·ªÉ y t·∫ø.\n",
                "\n",
                "## ENTITY TYPES:\n",
                "DISEASE, DRUG, SYMPTOM, TEST, ANATOMY, TREATMENT, PROCEDURE, RISK_FACTOR, LAB_VALUE\n",
                "\n",
                "## SCORING:\n",
                "- 9-10: Th·ª±c th·ªÉ CH√çNH (ƒë·ªÅ c·∫≠p nhi·ªÅu l·∫ßn)\n",
                "- 7-8: QUAN TR·ªåNG\n",
                "- 5-6: PH·ª§\n",
                "- 3-4: LI√äN QUAN nh·∫π\n",
                "\n",
                "## EXAMPLES:\n",
                "Input: \"B·ªánh th·∫≠n m·∫°n giai ƒëo·∫°n 5 c·∫ßn l·ªçc m√°u. Erythropoietin khi Hb < 10g/dL.\"\n",
                "Output:\n",
                "- B·ªánh th·∫≠n m·∫°n giai ƒëo·∫°n 5 (DISEASE, 10)\n",
                "- L·ªçc m√°u (TREATMENT, 9)\n",
                "- Erythropoietin (DRUG, 8)\n",
                "\n",
                "## RULES:\n",
                "1. GI·ªÆ NGUY√äN ti·∫øng Vi·ªát\n",
                "2. KH√îNG extract: s·ªë trang, quy·∫øt ƒë·ªãnh, vƒÉn b·∫£n\n",
                "\n",
                "Tr√≠ch xu·∫•t:\"\"\"),\n",
                "            (\"human\", \"{text}\")\n",
                "        ])\n",
                "        self._init_llm()\n",
                "    \n",
                "    def _init_llm(self):\n",
                "        self.llm = ChatGroq(temperature=0.1, model=\"llama-3.3-70b-versatile\", \n",
                "                           api_key=self.api_manager.get_current_key())\n",
                "        self.chain = self.extraction_prompt | self.llm.with_structured_output(AMGExtractionResult)\n",
                "    \n",
                "    def extract(self, text: str, max_retries=3):\n",
                "        for attempt in range(max_retries):\n",
                "            try:\n",
                "                result = self.chain.invoke({\"text\": text})\n",
                "                if result:\n",
                "                    # Filter\n",
                "                    valid = [e for e in result.entities \n",
                "                            if len(e.name) >= 2 and \n",
                "                            not any(x in e.name.lower() for x in ['quy·∫øt ƒë·ªãnh', 'vƒÉn b·∫£n', 'trang'])]\n",
                "                    result.entities = valid\n",
                "                return result if result else AMGExtractionResult()\n",
                "            except Exception as e:\n",
                "                if \"rate\" in str(e).lower() or \"limit\" in str(e).lower():\n",
                "                    if self.api_manager.rotate_key():\n",
                "                        self._init_llm()\n",
                "                        time.sleep(2)\n",
                "                    else:\n",
                "                        print(\"‚è≥ Ch·ªù 5 ph√∫t...\")\n",
                "                        time.sleep(300)\n",
                "                        self.api_manager.reset_failed()\n",
                "                        self._init_llm()\n",
                "                else:\n",
                "                    time.sleep(1)\n",
                "        return AMGExtractionResult()\n",
                "\n",
                "print(\"‚úÖ Extractor OK\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === HELPER: ADD TO GRAPH ===\n",
                "def add_entity(G, entity: MedicalEntity, page_num: int):\n",
                "    norm_name = normalize_medical_text(entity.name)\n",
                "    confidence = min(1.0, entity.relevance_score / 10.0)\n",
                "    \n",
                "    if not G.has_node(norm_name):\n",
                "        G.add_node(norm_name, label=entity.name, type=entity.type.upper(),\n",
                "                  description=entity.description, confidence=confidence,\n",
                "                  relevance_score=entity.relevance_score, pages=[page_num])\n",
                "    else:\n",
                "        old_conf = G.nodes[norm_name].get('confidence', 0)\n",
                "        if confidence > old_conf:\n",
                "            G.nodes[norm_name]['confidence'] = confidence\n",
                "            G.nodes[norm_name]['description'] = entity.description\n",
                "        if page_num not in G.nodes[norm_name]['pages']:\n",
                "            G.nodes[norm_name]['pages'].append(page_num)\n",
                "\n",
                "def add_relation(G, rel: MedicalRelation, page_num: int):\n",
                "    src = normalize_medical_text(rel.source_name)\n",
                "    tgt = normalize_medical_text(rel.target_name)\n",
                "    if G.has_node(src) and G.has_node(tgt):\n",
                "        G.add_edge(src, tgt, relation=rel.relation.upper(),\n",
                "                  confidence=min(1.0, rel.confidence_score/10), \n",
                "                  evidence=rel.evidence, page=page_num)\n",
                "\n",
                "print(\"‚úÖ Helpers OK\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === EXTRACT T·ª™ CHUNK 121 ===\n",
                "START_CHUNK = 120\n",
                "START_PAGE = 17\n",
                "\n",
                "print(f\"üöÄ RESUME T·ª™ CHUNK {START_CHUNK + 1}\\n\")\n",
                "\n",
                "# Load PDF\n",
                "loader = PyPDFLoader(PDF_PATH)\n",
                "pages = loader.load()[START_PAGE-1:]\n",
                "print(f\"‚úÇÔ∏è C·∫Øt {START_PAGE-1} trang, c√≤n {len(pages)} trang\")\n",
                "\n",
                "# Split\n",
                "splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
                "chunks = splitter.split_documents(pages)\n",
                "print(f\"üì¶ T·ªïng: {len(chunks)} chunks\")\n",
                "print(f\"‚ñ∂Ô∏è  X·ª≠ l√Ω: chunk {START_CHUNK + 1} ‚Üí {len(chunks)}\\n\")\n",
                "\n",
                "# Extract\n",
                "extractor = OptimizedAMGExtractor(api_manager)\n",
                "chunks_to_process = chunks[START_CHUNK:]\n",
                "\n",
                "print(\"üß† ƒêANG TR√çCH XU·∫§T (Optimized Prompts)...\\n\")\n",
                "\n",
                "for i, chunk in enumerate(chunks_to_process, start=START_CHUNK+1):\n",
                "    page_num = chunk.metadata.get(\"page\", 0) + 1\n",
                "    result = extractor.extract(chunk.page_content)\n",
                "    \n",
                "    if result and len(result.entities) > 0:\n",
                "        avg_rel = sum(e.relevance_score for e in result.entities) / len(result.entities)\n",
                "        print(f\"   [{i}/{len(chunks)}] +{len(result.entities)} entities (avg: {avg_rel:.1f})\")\n",
                "        \n",
                "        for ent in result.entities:\n",
                "            add_entity(G, ent, page_num)\n",
                "        \n",
                "        for rel in result.relations:\n",
                "            add_relation(G, rel, page_num)\n",
                "    else:\n",
                "        print(f\"   [{i}/{len(chunks)}] --\")\n",
                "    \n",
                "    # Save m·ªói 20 chunks\n",
                "    if i % 20 == 0:\n",
                "        with open(\"./amg_data/graph_full.pkl\", \"wb\") as f:\n",
                "            pickle.dump(G, f)\n",
                "        print(f\"   üíæ Saved: {G.number_of_nodes()} nodes\\n\")\n",
                "    \n",
                "    time.sleep(1)\n",
                "\n",
                "print(f\"\\n‚úÖ XONG!\")\n",
                "print(f\"Final: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === SAVE ===\n",
                "with open(\"./amg_data/graph_full.pkl\", \"wb\") as f:\n",
                "    pickle.dump(G, f)\n",
                "\n",
                "print(f\"üíæ ƒê√£ l∆∞u graph_full.pkl\")\n",
                "print(f\"   {G.number_of_nodes()} nodes | {G.number_of_edges()} edges\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === VISUALIZE ===\n",
                "net = Network(height=\"850px\", width=\"100%\", directed=True, notebook=False)\n",
                "net.force_atlas_2based(gravity=-50, spring_length=200)\n",
                "\n",
                "color_map = {\n",
                "    'DISEASE': '#ff6b6b', 'DRUG': '#4ecdc4', 'SYMPTOM': '#ffe66d',\n",
                "    'TEST': '#95e1d3', 'TREATMENT': '#aa96da'\n",
                "}\n",
                "\n",
                "for node, data in G.nodes(data=True):\n",
                "    color = color_map.get(data.get('type', 'OTHER'), '#97C2FC')\n",
                "    conf = data.get('confidence', 0.5)\n",
                "    size = 15 + conf * 25\n",
                "    net.add_node(node, label=data.get('label'), color=color, size=size,\n",
                "                title=f\"{data.get('label')}<br>Conf: {conf:.2f}\")\n",
                "\n",
                "for u, v, data in G.edges(data=True):\n",
                "    width = 1 + data.get('confidence', 0.5) * 3\n",
                "    net.add_edge(u, v, label=data.get('relation'), width=width)\n",
                "\n",
                "net.show_buttons(filter_=['physics'])\n",
                "net.write_html(\"Medical_Graph_Final.html\")\n",
                "\n",
                "print(\"‚úÖ Medical_Graph_Final.html\")\n",
                "print(\"üéâ HO√ÄN T·∫§T!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}